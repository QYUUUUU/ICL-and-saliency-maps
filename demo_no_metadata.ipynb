{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVJ93VW2pOwh"
   },
   "source": [
    "# Understanding ICL with saliency Demonstrations\n",
    "\n",
    "# Contrastive Demonstrations & Saliency Maps\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SMZ17B6NFs4"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOBg4Ots9juj"
   },
   "source": [
    "## 0. Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pMN5mhcp9k3I",
    "outputId": "4d732625-10ad-49f4-d887-ca9f8b41f685"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "!pip install  transformers datasets captum\n",
    "!pip install  scikit-learn numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqWL1X-H98EM"
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOkZfi3vpOwp"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from captum.attr import IntegratedGradients, Lime\n",
    "from captum.attr._core.lime import get_exp_kernel_similarity_function\n",
    "from captum._utils.models.linear_model import SkLearnLinearModel\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_202dLr-KxJ"
   },
   "source": [
    "## 1.1 Setup\n",
    "device selection depending on os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JrYL3ttU-OeE",
    "outputId": "fd352ce1-34df-4f52-dcbc-103d5f331ff3"
   },
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsOFMDmrpOwt"
   },
   "source": [
    "###  2. Load SST-2 and filter examples\n",
    "\n",
    "Load SST-2 dataset and initialize the LLMs,\n",
    "paper uses SST-2 and filters training examples to those with at least 20 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539,
     "referenced_widgets": [
      "39416d9289f147cf94ff23068502d5b2",
      "43a78b8aae554f37aa99aab844aafc0c",
      "d14978319fb94402ae2d1bdfaf6d0a26",
      "3de903d4190f4c53ae7099bb802643f9",
      "174346ca201242e8bf050a777435e226",
      "8dfb5603e8704068939c8722c2ced484",
      "7f1271e4fa174dc5870bba0def9e767c",
      "17d71aed0bae4e61980b1f3efd5d3bfc",
      "8de02490d6b544e9a4e3b30e06c13eb5",
      "fbc38fa4cfc64c31a0b3267527645418",
      "8058ea40229543abb179aa0d93d94a9d",
      "f994a5abb3114560a11392f55b1a0ee2",
      "6dcf47be5af6493897e3bf2784929eb5",
      "3cc42286734b4b6aaac9d8386feb636d",
      "be3d791550414b35949d35aa7b231a3e",
      "60898aec159e4bf6aac6272fadad1950",
      "209c410ffd904747a550175f073d2f66",
      "5535a2785639454db4e9caad54a0d3c1",
      "d521724e15674d3284a607743fa3e9b4",
      "5dad598a2f9e4c3daa1891e992d6c7ae",
      "0a2a12e5ecd44801b41ac57f608db109",
      "8fe6d911e76949418040f2ef7cbd3220",
      "21e47ad9f32e4b739ca74a2c92c154c4",
      "dbe1db9c3afa4a97a2ef945a6d5104b5",
      "e4798f0022a049f89547e35532558666",
      "7b729de94a3246a7a98ba018f7d2a3b1",
      "73fd5bac0a15445f89bae8307da715d0",
      "8bf01e2607ed49be85114447dcefb1c7",
      "15f512e9dc4441c7bb1abc6687410e0b",
      "3ea15c64997e4e819d999a9897ad9761",
      "70c92c8ab9f7419892e98d685d2afb26",
      "b0e595f550f3412091f8774d82bb9b67",
      "3da9f96d18f5416d8e356c25462f3271",
      "964d54a07ff443a5823541e67d01b4c8",
      "39501fd52c9042f4a7f006ba60c0307d",
      "0ffe2849ccf748599e9411287f5dc774",
      "e0bcefb90dd0490ab4c6de6d04f04bde",
      "c1152eb9e0d54733aa35e7954dfec330",
      "43a217920e264ec2b44c409e14e3d7a8",
      "5deb3d3558194a16b9062ba80d459202",
      "a79f6faeee2642448d7d50af2df71121",
      "a182e3ce58ca41f3b077be8b91b2cb46",
      "5109a775ee61470d923588ab4f0497a5",
      "dbf5d5e576ad47228ff94902c1aef90d",
      "8a8c6dcad86f4d789ea0ceb48cc4fdd2",
      "3e92f29951fc429e88bdf90e8a1ed0a6",
      "283a17bb67d7422389f3d1fca2746fe0",
      "bc27c30f8bc04cdeb5009f6f57ac8557",
      "9139cd9391af41b8bb9f6033ac07816b",
      "2c896eefdb3041c69878acf200bb8efb",
      "ed18f5aa0e9148b2af6dfd205994f49a",
      "ae4e3fe076824bb3aaa2fdf4733af511",
      "7becd348a2e149b0b215ac9fd3eb0ee8",
      "ef8fa0d25277452a92b68432931e0839",
      "5cd3ae8bb2d742349e222b04b2e650e1",
      "b26b4cf7afd04e488c249640a539eae4",
      "65e2504f438d40a2ba57a37f8c42fc31",
      "b1296a02ca2d4a1e8772743ac7022b58",
      "a6adaacf8f9a4976838a3d5269d0425d",
      "b07a409fe738407ea9852ab78f58155f",
      "ca2958acc1fd4fe3b67d1687d996d541",
      "931ceafb21c347ffae1d16916f791498",
      "99728ee5209e4552809c1e1c81fee4ea",
      "4c5f0a32ec6b496d87644fb1c7959a4c",
      "56a433ac048e4371afa479adbcee631c",
      "bbc8f54163a24fd0a777f902ee28632a",
      "74627f35bcfc4be98eb5afe36b9fd605",
      "655eeb636fcd4c54ad3fb8e18dbfdc2c",
      "7a13ab6aed6c4f86bc06b4632fa69b0a",
      "5ccb2bb1b7d04d399de1e9b7772edac0",
      "fa774702b0384d508ef53b818ab71581",
      "093f3728f30f47b68f664894caf64200",
      "9ed95532f35e41cd8390719af46d8bf6",
      "06e5eba612e74a77ad9bfc13ebe26664",
      "26336369e21d41b08ded97c532b7fefa",
      "dca7c4938f2743c595f46a7725c3dad5",
      "013f49cbd27c4256a878999acc8b9dad",
      "9ed85c8eb9424f6d95d4a3bcaa68049b",
      "90eb15c20fa94c8b96e28f6d904b9285",
      "721efa65ad7a46538d88187bdfb1eae0",
      "c35bf19eb7d74988bf92d19cfcec0ff3",
      "db8147bde4e5475ba0e828efed3852a6",
      "3c7fec234fa84ba2b8ec0cff8a140e6c",
      "47b157d460674bdd949bd2afb7777d10",
      "e673fdf73c704bfd8e3e050486cf81ff",
      "bb1a2add7b8841558a9ca6bc462c60bb",
      "3be54c1577be4c6198232cd03f0e813b",
      "f360285960ec4bc981f33cc470e62f0c",
      "18dfdedeb1cb4f9f83f940e4e0484d04",
      "10fbc5b7abec42ccaf17f367107a9857",
      "47c8ebf233574ed69af60bc103f5d395",
      "3a97ee3662f549fb8031b0247708539a",
      "649603216bcf428085e76457a706e3f1",
      "0b23b792e13b4dc0a6a032c04ca1f6d5",
      "becf35c69724438f8a722bc7085c3c20",
      "e13aff2fec9e4b6b99f011311e12c54b",
      "a9613e44a0744ceba6373bd301253cf1",
      "a16bd6ee27294e48af34882f30ef18ca",
      "2d4d3093be2342b1bce4c093868a1154"
     ]
    },
    "id": "1clZjfpFpOww",
    "outputId": "f8a2021a-49d5-4ef3-b717-4f5f839635e5"
   },
   "outputs": [],
   "source": [
    "# The dataset the paper used : https://huggingface.co/datasets/stanfordnlp/sst2\n",
    "\n",
    "# This code loads a dataset from the hugginface servers\n",
    "sst2 = load_dataset(\"glue\", \"sst2\")\n",
    "train_data = sst2[\"train\"]\n",
    "val_data = sst2[\"validation\"]\n",
    "\n",
    "print(\"Total training examples:\", len(train_data))\n",
    "print(\"Total validation examples:\", len(val_data))\n",
    "\n",
    "\n",
    "def word_count(text: str) -> int:\n",
    "    return len(text.split())\n",
    "\n",
    "# filter to sentences with >= 20 tokens (approximate to the paper)\n",
    "train_long = train_data.filter(lambda ex: word_count(ex[\"sentence\"]) >= 20)\n",
    "val_long = val_data.filter(lambda ex: word_count(ex[\"sentence\"]) >= 20)\n",
    "\n",
    "print(\"Long training examples (>=20 tokens):\", len(train_long))\n",
    "print(\"Long validation examples (>=20 tokens):\", len(val_long))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# build test set from filtered validation data\n",
    "test_data = list(val_long)\n",
    "\n",
    "# optionally limit size for speed\n",
    "MAX_TEST_SAMPLES = 250\n",
    "if len(test_data) > MAX_TEST_SAMPLES:\n",
    "    random.seed(42)\n",
    "    test_data = random.sample(test_data, MAX_TEST_SAMPLES)\n",
    "\n",
    "\n",
    "# We make sure that we have as many positives as negatives\n",
    "positives = [ex for ex in test_data if ex[\"label\"] == 1]\n",
    "negatives = [ex for ex in test_data if ex[\"label\"] == 0]\n",
    "\n",
    "balanced_size = min(len(positives), len(negatives))\n",
    "test_data = positives[:balanced_size] + negatives[:balanced_size]\n",
    "\n",
    "print(\"Test set size:\", len(test_data))\n",
    "print(\"Example test sentence:\", test_data[0][\"sentence\"], \"| Label:\", test_data[0][\"label\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fj9Xk9V1_Rme"
   },
   "source": [
    "## 3. Select 4 demonstration examples (2 positive, 2 negative)\n",
    "\n",
    "We want 2 strongly positive and 2 strongly negative reviews as demonstrations, similar to the paper (distinct sentiment-indicative words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcBgSrRj_xWl",
    "outputId": "8fd3f6b5-66fa-43fc-8fdf-0d1da3f58289"
   },
   "outputs": [],
   "source": [
    "positive_keywords = [\"great\", \"excellent\", \"amazing\", \"fantastic\", \"wonderful\", \"best\"]\n",
    "negative_keywords = [\"terrible\", \"awful\", \"horrible\", \"worst\", \"boring\", \"waste\", \"bad\"]\n",
    "\n",
    "pos_candidates = [\n",
    "    ex for ex in train_long\n",
    "    if ex[\"label\"] == 1 and any(w in ex[\"sentence\"].lower() for w in positive_keywords)\n",
    "]\n",
    "neg_candidates = [\n",
    "    ex for ex in train_long\n",
    "    if ex[\"label\"] == 0 and any(w in ex[\"sentence\"].lower() for w in negative_keywords)\n",
    "]\n",
    "\n",
    "print(f\"Positive candidates: {len(pos_candidates)}\")\n",
    "print(f\"Negative candidates: {len(neg_candidates)}\")\n",
    "\n",
    "# pick first two of each\n",
    "assert len(pos_candidates) >= 2 and len(neg_candidates) >= 2, \"Not enough candidates; try adjusting keywords.\"\n",
    "\n",
    "demo_pos1 = pos_candidates[0]\n",
    "demo_pos2 = pos_candidates[1]\n",
    "demo_neg1 = neg_candidates[0]\n",
    "demo_neg2 = neg_candidates[1]\n",
    "\n",
    "print(\"Positive demo 1:\", demo_pos1[\"sentence\"])\n",
    "print(\"Positive demo 2:\", demo_pos2[\"sentence\"])\n",
    "print(\"Negative demo 1:\", demo_neg1[\"sentence\"])\n",
    "print(\"Negative demo 2:\", demo_neg2[\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2pLuljJpOw7"
   },
   "source": [
    "## 4. Build contrastive demonstrations\n",
    "\n",
    "4 demos:\n",
    "\n",
    "1. Original (correct label) and original language\n",
    "2. Label-flipped\n",
    "3. Input neutralized (adjectives toned down, label unchanged)\n",
    "4. With explanations (label plus explanation line)\n",
    "\n",
    "We’ll implement a simple neutralization (rule-based replacements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqYmy4mOpOw9",
    "outputId": "d245bb3e-8d28-4548-c263-8752db726e33"
   },
   "outputs": [],
   "source": [
    "demo_examples = [\n",
    "    {\"text\": demo_pos1[\"sentence\"], \"label\": \"Positive\"},\n",
    "    {\"text\": demo_pos2[\"sentence\"], \"label\": \"Positive\"},\n",
    "    {\"text\": demo_neg1[\"sentence\"], \"label\": \"Negative\"},\n",
    "    {\"text\": demo_neg2[\"sentence\"], \"label\": \"Negative\"},\n",
    "]\n",
    "\n",
    "def neutralize_text(text: str) -> str:\n",
    "    # rule-based neutralization\n",
    "    replacements = {\n",
    "        # Stronger neutralization for positive adjectives/adverbs\n",
    "        \"great\": \"okay\", \"excellent\": \"acceptable\", \"amazing\": \"moderate\",\n",
    "        \"fantastic\": \"average\", \"wonderful\": \"ordinary\", \"best\": \"common\",\n",
    "        \"outstanding\": \"decent\", \"superb\": \"adequate\", \"perfect\": \"sufficient\",\n",
    "        \"brilliant\": \"fine\", \"incredible\": \"average\", \"awesome\": \"moderate\",\n",
    "        \"impressive\": \"okay\", \"remarkable\": \"decent\", \"enjoyable\": \"fine\",\n",
    "        \"fabulous\": \"ordinary\", \"splendid\": \"acceptable\", \"marvelous\": \"average\",\n",
    "        \"pleasant\": \"moderate\", \"delightful\": \"ordinary\", \"exceptional\": \"average\",\n",
    "        \"terrific\": \"okay\", \"lovely\": \"fine\", \"phenomenal\": \"moderate\",\n",
    "\n",
    "        # Negative adjectives/adverbs (keep existing)\n",
    "        \"terrible\": \"very\", \"awful\": \"rather\", \"horrible\": \"really\", \"worst\": \"quite\",\n",
    "        \"boring\": \"uneventful\", \"waste\": \"useful\", \"bad\": \"not good\",\n",
    "        \"dreadful\": \"subpar\", \"lousy\": \"mediocre\", \"poor\": \"average\",\n",
    "        \"unpleasant\": \"okay\", \"horrid\": \"acceptable\", \"mediocre\": \"decent\",\n",
    "        \"annoying\": \"slightly irritating\", \"disappointing\": \"somewhat off\",\n",
    "        \"frustrating\": \"challenging\", \"uninteresting\": \"neutral\",\n",
    "        \"forgettable\": \"ordinary\", \"weak\": \"not strong\", \"substandard\": \"acceptable\",\n",
    "        \"pathetic\": \"mediocre\", \"inferior\": \"average\",\n",
    "\n",
    "        # Intensifiers / adverbs\n",
    "        \"very\": \"slightly\", \"extremely\": \"moderately\", \"highly\": \"fairly\",\n",
    "        \"so\": \"somewhat\", \"too\": \"a bit\", \"incredibly\": \"moderately\",\n",
    "        \"exceptionally\": \"moderately\", \"really\": \"somewhat\", \"absolutely\": \"fairly\",\n",
    "        \"completely\": \"somewhat\", \"totally\": \"moderately\",\n",
    "\n",
    "        # Common positive phrases\n",
    "        \"loved it\": \"liked it\", \"highly recommend\": \"recommend\",\n",
    "        \"must watch\": \"worth watching\", \"top-notch\": \"average\",\n",
    "        \"five stars\": \"three stars\", \"well done\": \"okay\",\n",
    "        \"thumbs up\": \"neutral\", \"great job\": \"okay job\",\n",
    "\n",
    "        # Common negative phrases\n",
    "        \"hated it\": \"didn't like it\", \"do not recommend\": \"recommend cautiously\",\n",
    "        \"waste of time\": \"not very engaging\", \"one star\": \"three stars\",\n",
    "        \"poorly done\": \"okay effort\", \"could be better\": \"average\",\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    result = text\n",
    "    for w, n in replacements.items():\n",
    "        result = result.replace(w, n)\n",
    "        result = result.replace(w.capitalize(), n.capitalize())\n",
    "    return result\n",
    "\n",
    "\n",
    "# Falling short of being able to generate the context explanation using GPT4, and then refining those explanations manually for hundreds of examples,\n",
    "# we will instead write a generic explanation for the positive and for the negative movie reviews that do no contain appreciative adjectives\n",
    "explanations = []\n",
    "for ex in demo_examples:\n",
    "    if ex[\"label\"] == \"Positive\":\n",
    "        explanations.append(\"The review describes an experience where the viewer was able to follow and interpret the events in the film as they occurred.\")\n",
    "    else:\n",
    "        explanations.append(\"The review describes an experience where the viewer encountered difficulties in following or interpreting some events in the film.\")\n",
    "\n",
    "original_demos = []\n",
    "flipped_demos = []\n",
    "neutral_demos = []\n",
    "explained_demos = []\n",
    "\n",
    "for i, ex in enumerate(demo_examples):\n",
    "    text = ex[\"text\"].strip()\n",
    "    orig_label = ex[\"label\"]\n",
    "    flip_label = \"Negative\" if orig_label == \"Positive\" else \"Positive\"\n",
    "    neut_text = neutralize_text(text)\n",
    "    explanation = explanations[i]\n",
    "\n",
    "    original_demos.append(f\"Review: {text}\\nSentiment: {orig_label}\")\n",
    "    flipped_demos.append(f\"Review: {text}\\nSentiment: {flip_label}\")\n",
    "    neutral_demos.append(f\"Review: {neut_text}\\nSentiment: {orig_label}\")\n",
    "    explained_demos.append(\n",
    "        f\"Review: {text}\\nSentiment: {orig_label}\\nExplanation: {explanation}\"\n",
    "    )\n",
    "\n",
    "print(\"Original demo 1:\\n\", original_demos[0], \"\\n\")\n",
    "print(\"Flipped demo 1:\\n\", flipped_demos[0], \"\\n\")\n",
    "print(\"Neutralized demo 1:\\n\", neutral_demos[0], \"\\n\")\n",
    "print(\"Explanation demo 1:\\n\", explained_demos[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNNP5hD_pOxB"
   },
   "source": [
    "## 5. Prompt builder for in-context learning\n",
    "\n",
    "We put 4 demos then the test review, and leave Sentiment: blank for GPT-2 to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hqn1Lh3qpOxE",
    "outputId": "ec0b05cf-84ec-44aa-e7c3-976dc40fc55e"
   },
   "outputs": [],
   "source": [
    "def build_prompt(demo_list, test_sentence: str) -> str:\n",
    "    prompt = \"\"\n",
    "    for demo in demo_list:\n",
    "        prompt += demo.strip() + \"\\n\\n\"\n",
    "    prompt += f\"Review: {test_sentence.strip()}\\nSentiment: \"\n",
    "    return prompt\n",
    "\n",
    "# prompt examples\n",
    "sample_prompt = build_prompt(original_demos, test_data[0][\"sentence\"])\n",
    "print(sample_prompt)\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "sample_prompt = build_prompt(explained_demos, test_data[0][\"sentence\"])\n",
    "print(sample_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfZGGTQ-pOxG"
   },
   "source": [
    "## 6. Load GPT-2 & implement ICL prediction\n",
    "\n",
    "We’ll decode the next token and interpret it as “Positive”/“Negative”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259,
     "referenced_widgets": [
      "f8b0599c3e97435091f809746d8f5775",
      "e921786e53c4424a81c07eb9b665f916",
      "880ddbb54ee140c4a3926981ff4b3871",
      "2bc2703528f348f1bcbcba1409b64e6c",
      "a7fa9791990d4160a280e8a16bcd0acd",
      "69112679097340bcb59f1bb9c6b23cdf",
      "4765d7d8f85a4157b39cfc91cd39d6b5",
      "4ede5271b1034a10bd91e2fa88b04895",
      "9c11c08de178466c80eedff2c1483bab",
      "004d68e921034d48bc452ea1c82d4cd4",
      "b0131da584c146fc8ee8318b0b49aefe",
      "4efc4858221940f19466c45de1b6af5d",
      "c1b8ea86c0f14c8183d8dd3b1cfd51e4",
      "64dd31e2b17349aaa155392293f6f726",
      "b68ec8b8e0cc4a02926d68033dcc836f",
      "e0b31ac657e643d7802181e951197d8a",
      "c007da8ccef84988a9b5d6be0f0e6670",
      "b13c14dfb4784026935488a1053a4fcc",
      "b18d173a9a4c4880b74503875c1f47ab",
      "3c7d7afa5b9b48e2b5c4b31a2e5bca2e",
      "ceb7c66678774c53842874a841cf90e4",
      "e0ffd73cbdbe43ff84637f6c2110d49f",
      "f4a9577c56b349c498de7144b78f4cb0",
      "7b3725c07953452da019dc43b588e9b4",
      "71d1376fd7ae4a77b113325b393fd921",
      "9dfbd3fac03743089d608b74776d41bd",
      "3b01c626088047c8acb4ebf62a165af6",
      "73f1309288ba4530a65c5f6bf6639c2b",
      "619fdbd2ddd444008de0ddf79e528885",
      "aba061a369d34797827b7327ca96ad4e",
      "5bc7f21cd7c94050bcc92f9f3c5a9c94",
      "1b36e759bac2439e90b0d701696fd461",
      "4f9249740d9a4ebdab7c3745ac906ab4",
      "3589e63cab8947a0af41df91d03c8ccf",
      "48078f37cabc49a7bbee07821bc724ca",
      "daf17b42cef148cabff34ba73659b1b5",
      "dee3e2f1ac834141a9ef96684bb5af20",
      "77ff1f5d59b34369be777c13135c808f",
      "3b101aeda524438a8a6c1599f94935aa",
      "ce508c105f8748488c9997cfb9b3384c",
      "5b14726c688941f090d0e27476e85e17",
      "f2430a656fa04da8a168634df1524958",
      "4fd8a5f54b7140ba8fa0b3848e172abf",
      "bc3945a13af7437bae227bed4dc926cd",
      "c5e975de50e841aaabe88411bb3ac707",
      "4ae6fe1c2f8a40ea9afbeaef0e4c58d7",
      "8e75fe5a9d82487dbba0179004bd396e",
      "f5118aa141f84537a2775c3f280309f6",
      "24cd97bba0c7432ca1a9bee475c37651",
      "de60f2c003a9447c804cccd72bb847c6",
      "52accad428a54ac99266afa3130d8c44",
      "2e28c7613ee34d4195923c7159615543",
      "6a4294b3c403431d8bd0a05165a7790f",
      "01fcb11c68d249a4999207473d925069",
      "891c78e0d2f444b99b9324056851ada4",
      "e28eaf58ffb7403ab60abda160b3d847",
      "94920e0d2f574e7e98a2076c0a719b5a",
      "09ddb85728ce4cdd8daaae2b69b37e04",
      "a20bf93cfc584ed08e527d426eab19a9",
      "f6d8123bedd94559877e83d15547997a",
      "00c72e558ad643b081c7fc690090aaf7",
      "8c7f23136e354f6bb82767242bec9a26",
      "6f436bf2cbe94875b80fbee778e48b11",
      "289d4680bb034332b0e1d7694e9fa6d9",
      "c9407c1f91764ad7bbc975213788abf2",
      "7b8cbf04f8284eacbaea64c2eadc4546",
      "9102b14b7f974b8eb32b8b4332b3dcee",
      "95320d667d3d48c5ba4db917b5308ad5",
      "3f30184150f74fceb20508541518fa10",
      "d7c5296a330a431d9a9fa63700348af6",
      "8d150679dcbd4add879d4b1d9375ef68",
      "b3a41f7c5c234afd937233c95db1dc0b",
      "31d0319e230f4df9a949e5700cb348cc",
      "007e6f6814524fd9a4899599cfaf7054",
      "387f58bdb64047f0b8be5da4d2223a04",
      "b7969b92d5c3464990cdb0039eadd3b2",
      "7e72766fa802442eb078abadcab6246a"
     ]
    },
    "id": "LZy65y-vAxFb",
    "outputId": "19fc5345-3389-4e39-9766-4f8bff2c8b46"
   },
   "outputs": [],
   "source": [
    "# Load GPT-2\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "gpt2_model.eval()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Clean parser: extract \"Positive\" or \"Negative\" from a text\n",
    "# -----------------------------------------------------------\n",
    "def extract_label(text: str) -> str:\n",
    "    t = text.strip().lower()\n",
    "\n",
    "    # strict matching\n",
    "    if t == \"positive\":\n",
    "        return \"Positive\"\n",
    "    if t == \"negative\":\n",
    "        return \"Negative\"\n",
    "\n",
    "    # looser matching if output has extra tokens\n",
    "    if \"positive\" in t:\n",
    "        return \"Positive\"\n",
    "    if \"negative\" in t:\n",
    "        return \"Negative\"\n",
    "\n",
    "    return \"Negative\"   # fallback\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# GPT-2 sentiment classifier via generation\n",
    "# -----------------------------------------------------------\n",
    "def predict_gpt2_sentiment(prompt: str) -> str:\n",
    "    \"\"\"Force GPT-2 to *generate* a label and then parse it.\"\"\"\n",
    "\n",
    "    inputs = gpt2_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = gpt2_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=3,          # enough to get \"Positive\" or \"Negative\"\n",
    "            do_sample=False,           # deterministic\n",
    "            pad_token_id=gpt2_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    full_text = gpt2_tokenizer.decode(generated[0])\n",
    "\n",
    "    # Get only what GPT-2 produced *after* the prompt\n",
    "    completion = full_text[len(prompt):].strip()\n",
    "\n",
    "    return extract_label(completion)\n",
    "\n",
    "\n",
    "# Quick test\n",
    "test_prompt = (\n",
    "    \"Review: I really loved this movie. It was exciting and beautiful.\\n\"\n",
    "    \"Sentiment (Positive/Negative): \"\n",
    ")\n",
    "\n",
    "print(\"GPT-2 test prediction:\", predict_gpt2_sentiment(test_prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRnFymZhpOxK"
   },
   "source": [
    "## 7. Evaluate GPT-2 under the four demo conditions\n",
    "\n",
    "We’ll compute accuracy for:\n",
    "\n",
    "- Original demos\n",
    "\n",
    "- Label-flipped demos\n",
    "\n",
    "- Neutralized input demos\n",
    "\n",
    "- Explanation demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EadxwoiwBLVs",
    "outputId": "28da3baa-ac67-4451-fc46-140ee765dc9b"
   },
   "outputs": [],
   "source": [
    "label_map = {\"Positive\": 1, \"Negative\": 0}\n",
    "conditions = [\"Original\", \"Label-Flipped\", \"Neutralized\", \"With-Explanation\", \"No-ICL\"]\n",
    "\n",
    "# Initialize structures\n",
    "correct_counts = {c: 0 for c in conditions}\n",
    "all_preds = {c: [] for c in conditions}   # store predicted labels\n",
    "all_trues = []                            # store true labels\n",
    "\n",
    "for ex in test_data:\n",
    "    true_label = ex[\"label\"]\n",
    "    sent = ex[\"sentence\"]\n",
    "\n",
    "    all_trues.append(true_label)\n",
    "\n",
    "    prompts = {\n",
    "        \"Original\": build_prompt(original_demos, sent),\n",
    "        \"Label-Flipped\": build_prompt(flipped_demos, sent),\n",
    "        \"Neutralized\": build_prompt(neutral_demos, sent),\n",
    "        \"With-Explanation\": build_prompt(explained_demos, sent),\n",
    "        \"No-ICL\": f\"You are GPT2, you are classifying a review's sentiment. It is either \\\"Positive\\\" or \\\"Negative\\\".\\n Review: \\\"{sent}\\\"\\nSentiment:\"\n",
    "    }# without examples, the prompt becomes very important\n",
    "\n",
    "    for cond, prompt in prompts.items():\n",
    "        pred_text = predict_gpt2_sentiment(prompt)\n",
    "\n",
    "        if pred_text not in label_map:\n",
    "            print(f\"Unexpected prediction for condition '{cond}': {repr(pred_text)}\")\n",
    "            # You can still assign a default or skip counting\n",
    "            pred_label = None\n",
    "        else:\n",
    "            pred_label = label_map[pred_text]  # safe now\n",
    "            all_preds[cond].append(pred_label)\n",
    "\n",
    "            if pred_label == true_label:\n",
    "                correct_counts[cond] += 1\n",
    "\n",
    "\n",
    "# Print basic results\n",
    "print(\"\\nCorrect counts:\", correct_counts)\n",
    "\n",
    "accuracy = {cond: correct_counts[cond] / len(test_data) for cond in conditions}\n",
    "print(\"\\nGPT-2 accuracy:\")\n",
    "for cond, acc in accuracy.items():\n",
    "    print(f\"{cond}: {acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6bn1VOxnpEuR",
    "outputId": "5c99c3f0-9649-4a25-aa5e-d843357191df"
   },
   "outputs": [],
   "source": [
    "# Our neutralization method doesn't seem to have the same effect the paper had.\n",
    "# Let's investigate why it could be by making confusion matricies\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for cond in conditions:\n",
    "    cm = confusion_matrix(all_trues, all_preds[cond])\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\",\"Positive\"], yticklabels=[\"Negative\",\"Positive\"])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"GPT-2 Confusion Matrix - {cond}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUNHP619svMN"
   },
   "source": [
    "##Analysis of the confusion matrices\n",
    "As we can see, the Input neutralization skewed the model towards predicting a \"Positive\" label more often regardless of ground truth.\n",
    "This effect hints at the model's original bias that mostly classifies reviews as \"Negative\", and at the fact that our neutralization method fails to capture the subtle details in which a review can be positive.\n",
    "We therefore fail at replicating the findings of the paper.\n",
    "\n",
    "##Summary :\n",
    "We successfully showed that for a small model, label flipping has a detrimental effect on the predictions accuracy, and that CE has a minimal effect (even if its design is not review specific).\n",
    "\n",
    "##What's next in this demonstration\n",
    "Because of resource restrictions, we won't be able to make the comparisons between the small and bigger models, so we will continue the analysis of the ICL by using the saliency maps, and seeing how the model distributes its attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucpwwg4TBPF0"
   },
   "source": [
    " ## 8. BERT baseline: supervised DistilBERT on SST-2\n",
    "\n",
    "We’ll use Hugging Face’s pre-fine-tuned SST-2 DistilBERT as an example of what a model of this size could achieve without ICL and just fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254,
     "referenced_widgets": [
      "265c23a8116f4cb58546e159d78fca37",
      "7000d566fb9a4bdeb07b984e3b08d058",
      "b363086972b54b2ba4a539b804656f97",
      "bd300a1d4c13469fa283fd435eb58b0d",
      "d8fdb1e713b5413aa80f545e08c4f74f",
      "9f0f5e01f0904f77b0aa1c1899827288",
      "46cc579a4543449fb3bdb151756b419e",
      "f03b86205f6b43eaafaef548fc51e166",
      "1e7d882983ae40ff874609fe44b36d31",
      "6509357b036242de8d623615d6f498fb",
      "e1acc0692e68415db92f15fd181c922c",
      "68b9edf621f9427fba22ab8ff162cfab",
      "23c49263cb92455ab5b33ca7bee78a99",
      "7a44b1a6074849ea95c9dac47826f66e",
      "2578be8e45eb4a139e59ad29ac317662",
      "a42dd81072de45418328ece24b73267e",
      "88686ba240fa485288a573791820d1d0",
      "9facd6c366a4429b82fa46bc8ef3b0db",
      "591f62c0a86747048ec0d4a21cdda728",
      "ae978f4f81f34d04b7949d696b6245b0",
      "f71b51a84180413b96c0a00b08e3a9bb",
      "528df26f27f8404b8bdd2ac19a46327d",
      "996033c5a86240b8b909a3117cda367b",
      "16f8d9564cf84b1fb4511ebab7599a5f",
      "ae23ccb7ff9b4976838451608eb28df5",
      "e088b1dccd5a4898b37f39c1ede468da",
      "89e30084b0d148b4a667925e881efc1d",
      "31524e9ddb4e462d831317f613f2c7db",
      "4789dd963aee46229d38d738d86147d0",
      "da6bfb19350044a1be2a4c42cf58b33d",
      "1ec0e7708b84488c8dd20ab4f07af219",
      "2b6cf8409b0745169609170db38bbf24",
      "998e8eb0a2b149b1b2857a303722baa3",
      "7a3b281077a64ae0af41e72d854249ad",
      "452835d25c7d4ea4abac333c923d4ee6",
      "bc761ad5c74444d78da348b33b76773c",
      "e9878b47e1224857b3b36f1fc80d9b86",
      "8ced9079daaa47a9b716e20c925e0fe6",
      "7c4247ad0dbd4839b6a5211f8796fc21",
      "7ac68a909616463c82282e8d228f4d4d",
      "42891f97653242feb53ce02c675d9833",
      "4563e2bbfcac4c61ab09c2ae2f20c502",
      "bf399c4c6b524eeaa42ec6eaea70ae48",
      "0e5db13da22b4ea7868231697b8a19c6"
     ]
    },
    "id": "tZJzBuDIBTGr",
    "outputId": "cb1894ea-1fbc-4182-c8fd-47e0ac0e8246"
   },
   "outputs": [],
   "source": [
    "bert_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    return_all_scores=True,\n",
    ")\n",
    "\n",
    "bert_correct = 0\n",
    "for ex in test_data:\n",
    "    text = ex[\"sentence\"]\n",
    "    true_label = ex[\"label\"]\n",
    "    scores = bert_classifier(text)[0]\n",
    "    # scores is list of [{'label': 'NEGATIVE', 'score': ...}, {'label': 'POSITIVE', 'score': ...}]\n",
    "    best = max(scores, key=lambda x: x[\"score\"])\n",
    "    pred = 1 if best[\"label\"].upper() == \"POSITIVE\" else 0\n",
    "    if pred == true_label:\n",
    "        bert_correct += 1\n",
    "\n",
    "bert_acc = bert_correct / len(test_data)\n",
    "print(f\"DistilBERT fine-tuned accuracy: {bert_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fReq1HMFBXZM"
   },
   "source": [
    "## 9. Accuracy table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "YMOjfgL7Bcbl",
    "outputId": "ddb8ad54-c386-43b3-eff7-1f5c39c59682"
   },
   "outputs": [],
   "source": [
    "acc_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Model / Condition\": [\n",
    "            \"GPT-2 With ICL\",\n",
    "            \"GPT-2 Label-Flipped\",\n",
    "            \"GPT-2 Neutralized\",\n",
    "            \"GPT-2 + Explanation\",\n",
    "            \"GPT-2 Without ICL\",\n",
    "            \"BERT fine-tuned\",\n",
    "        ],\n",
    "        \"Accuracy (%)\": [\n",
    "            accuracy[\"Original\"] * 100,\n",
    "            accuracy[\"Label-Flipped\"] * 100,\n",
    "            accuracy[\"Neutralized\"] * 100,\n",
    "            accuracy[\"With-Explanation\"] * 100,\n",
    "            accuracy[\"No-ICL\"] * 100,\n",
    "            bert_acc * 100,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "acc_df.style.format({\"Accuracy (%)\": \"{:.2f}\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3umacuMBiWi"
   },
   "source": [
    "## 10. Captum: Integrated Gradients for GPT-2 prompts\n",
    "\n",
    "We now compute IG for one example across all four demo conditions.\n",
    "\n",
    "### 10.1. Picking an example GPT-2 understands correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fq46QoHrBmyB",
    "outputId": "3e5334f7-3152-446b-8553-73679dc4013e"
   },
   "outputs": [],
   "source": [
    "example_index = None\n",
    "for idx, ex in enumerate(test_data):\n",
    "    prompt = build_prompt(original_demos, ex[\"sentence\"])\n",
    "    pred = predict_gpt2_sentiment(prompt)\n",
    "    true_label = \"Positive\" if ex[\"label\"] == 1 else \"Negative\"\n",
    "    if pred == true_label:\n",
    "        example_index = idx\n",
    "        break\n",
    "\n",
    "if example_index is None:\n",
    "    example_index = 0\n",
    "\n",
    "example = test_data[example_index]\n",
    "print(\"Selected example index:\", example_index)\n",
    "print(\"Sentence:\", example[\"sentence\"])\n",
    "print(\"Label:\", example[\"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NEtvjJxBse_"
   },
   "source": [
    "### 10.2. Define GPT-2 forward function for Captum\n",
    "\n",
    "We’ll attribute the probability of the next token being \" Positive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d-gYNfR2B2g-"
   },
   "outputs": [],
   "source": [
    "# token ids for \"Positive\"\n",
    "pos_ids = gpt2_tokenizer.encode(\" Positive\", add_special_tokens=False)\n",
    "positive_id = pos_ids[0]\n",
    "positive_id\n",
    "\n",
    "\n",
    "\n",
    "def gpt2_forward_for_ig(inputs_embeds: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "\n",
    "    outputs = gpt2_model(\n",
    "        inputs_embeds=inputs_embeds.to(device),\n",
    "        attention_mask=attention_mask.to(device) if attention_mask is not None else None,\n",
    "    )\n",
    "    logits = outputs.logits[:, -1, :]   # (batch, vocab_size)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    pos_prob = probs[:, positive_id]    # (batch,)\n",
    "    return pos_prob\n",
    "\n",
    "\n",
    "\n",
    "ig = IntegratedGradients(gpt2_forward_for_ig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIA3EufxCEr4"
   },
   "source": [
    "### 10.3. Helper to run IG on a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zAJ6yMVECF3p"
   },
   "outputs": [],
   "source": [
    "def encode_for_ig(prompt: str):\n",
    "    enc = gpt2_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"].to(device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(device).long()\n",
    "    inputs_embeds = gpt2_model.transformer.wte(input_ids)\n",
    "    return input_ids, inputs_embeds, attention_mask\n",
    "\n",
    "def ig_attribution_for_prompt(prompt: str, n_steps: int = 50):\n",
    "    input_ids, inputs_embeds, attention_mask = encode_for_ig(prompt)\n",
    "    baseline_ids = torch.full_like(input_ids, gpt2_tokenizer.eos_token_id, dtype=torch.long, device=device)\n",
    "    baseline_embeds = gpt2_model.transformer.wte(baseline_ids)\n",
    "\n",
    "    attributions = ig.attribute(\n",
    "        inputs=inputs_embeds,\n",
    "        baselines=baseline_embeds,\n",
    "        additional_forward_args=(attention_mask,),\n",
    "        n_steps=n_steps,\n",
    "        internal_batch_size=1,\n",
    "    )  # (1, seq_len, hidden)\n",
    "\n",
    "    token_attrib = attributions.sum(dim=-1)  # sum over embedding dims -> (1, seq_len)\n",
    "    tokens = gpt2_tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    scores = token_attrib[0].detach().cpu().numpy()\n",
    "\n",
    "    abs_scores = np.abs(scores)\n",
    "    maxval = abs_scores.max() if abs_scores.max() != 0 else 1.0\n",
    "    norm_scores = abs_scores / maxval\n",
    "    return tokens, norm_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_pYZWlBCJ5-"
   },
   "source": [
    "### 10.4. Compute IG for all four demo conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AWqVBMV5CLAF",
    "outputId": "38e4e5dc-5b43-4250-a573-2f98eda1cfd0"
   },
   "outputs": [],
   "source": [
    "ex_sentence = example[\"sentence\"]\n",
    "\n",
    "prompts = {\n",
    "    \"Original\": build_prompt(original_demos, ex_sentence),\n",
    "    \"Label-Flipped\": build_prompt(flipped_demos, ex_sentence),\n",
    "    \"Neutralized\": build_prompt(neutral_demos, ex_sentence),\n",
    "    \"Explanation\": build_prompt(explained_demos, ex_sentence),\n",
    "}\n",
    "\n",
    "ig_results = {}\n",
    "for cond, p in prompts.items():\n",
    "    print(f\"Computing IG for condition: {cond}\")\n",
    "    tokens, scores = ig_attribution_for_prompt(p)\n",
    "    ig_results[cond] = (tokens, scores)\n",
    "    print(\"Prediction:\", predict_gpt2_sentiment(p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hThe14NCQAZ"
   },
   "source": [
    "### 10.5. HTML visualization of token saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dWkYSm3LCSZZ",
    "outputId": "9d2dd0b6-5d6c-423a-9a27-c66e1507bb49"
   },
   "outputs": [],
   "source": [
    "# Visualization function\n",
    "def highlight_tokens(tokens, scores):\n",
    "    spans = []\n",
    "    for tok, s in zip(tokens, scores):\n",
    "        if tok == gpt2_tokenizer.eos_token:\n",
    "            continue\n",
    "        s = max(0.0, min(1.0, s))\n",
    "        opacity = 0.05 + 0.9 * s\n",
    "        color = f\"rgba(255, 0, 0, {opacity:.2f})\"\n",
    "        safe_tok = tok.replace(\"<\", \"&#266;\").replace(\">\", \"&gt;\")\n",
    "        if safe_tok.startswith((\"Ġ\", \"Ċ\")):\n",
    "            safe_tok = safe_tok[1:]\n",
    "            spans.append(\" \")\n",
    "        spans.append(\n",
    "            f\"<span style='background-color:{color}; color:black; padding:2px 3px; border-radius:3px; margin:0 1px; font-weight:500;'>{safe_tok}</span>\"\n",
    "        )\n",
    "    return \"\".join(spans)\n",
    "\n",
    "#to identify CE tokens\n",
    "all_explanations = [\n",
    "    \"The review describes an experience where the viewer was able to follow and interpret the events in the film as they occurred.\",\n",
    "    \"The review describes an experience where the viewer encountered difficulties in following or interpreting some events in the film.\"\n",
    "]\n",
    "\n",
    "def get_label_ce_tokens(tokens, condition):\n",
    "    label_tokens = []\n",
    "    ce_tokens = []\n",
    "\n",
    "    # Normalize tokens\n",
    "    token_texts = [tok.replace(\"Ġ\",\"\").replace(\"Ċ\",\"\").lower() for tok in tokens]\n",
    "\n",
    "    # Labels\n",
    "    for i, tok in enumerate(token_texts):\n",
    "        if tok in [\"positive\", \"negative\"]:\n",
    "            label_tokens.append(i)\n",
    "\n",
    "    # CE tokens (condition name *must* match your dictionary)\n",
    "    if condition.lower() in [\"explanation\", \"with-explanation\"]:\n",
    "        for ce_sentence in all_explanations:\n",
    "            # tokenize CE sentence using GPT-2 tokenizer (critical!)\n",
    "            ce_tokens_raw = gpt2_tokenizer.tokenize(ce_sentence)\n",
    "\n",
    "            # Clean same as token_texts\n",
    "            ce_tokens_clean = [\n",
    "                t.replace(\"Ġ\",\"\").replace(\"Ċ\",\"\").lower()\n",
    "                for t in ce_tokens_raw\n",
    "            ]\n",
    "\n",
    "            L = len(ce_tokens_clean)\n",
    "\n",
    "            # sliding window over model input tokens\n",
    "            for i in range(len(token_texts) - L + 1):\n",
    "                if token_texts[i:i+L] == ce_tokens_clean:\n",
    "                    ce_tokens.extend(range(i, i+L))\n",
    "\n",
    "    return label_tokens, ce_tokens\n",
    "\n",
    "# Identify neutralized words by diffing Original vs Neutralized tokens\n",
    "def get_neutralized_indices(orig_tokens, neut_tokens):\n",
    "    # Simplest approach: mark tokens that are different at the same position\n",
    "    min_len = min(len(orig_tokens), len(neut_tokens))\n",
    "    neutralized_indices = []\n",
    "    for i in range(min_len):\n",
    "        if orig_tokens[i] != neut_tokens[i]:\n",
    "            neutralized_indices.append(i)\n",
    "    return neutralized_indices\n",
    "\n",
    "# Compute saliency tables\n",
    "saliency_summary = {}\n",
    "percent_summary = {}\n",
    "neutralized_saliency_comparison = {}\n",
    "\n",
    "for cond, (tokens, scores) in ig_results.items():\n",
    "    html_str = highlight_tokens(tokens, scores)\n",
    "    display(HTML(f\"\"\"\n",
    "        <div style='background-color:#f5f5f5; padding:15px; border-radius:5px; margin-bottom:20px;'>\n",
    "            <h3 style='color:#333; margin-top:0;'>{cond} prompt (IG saliency)</h3>\n",
    "            <div style='line-height:1.8em; font-size:14px;'>{html_str}</div>\n",
    "        </div>\n",
    "    \"\"\"))\n",
    "\n",
    "    label_tokens, ce_tokens = get_label_ce_tokens(tokens, cond)\n",
    "    neutralized_tokens = []\n",
    "\n",
    "    # for \"Neutralized\" find words changed from Original\n",
    "    if cond == \"Neutralized\":\n",
    "        orig_tokens = ig_results[\"Original\"][0]\n",
    "        neutralized_tokens = get_neutralized_indices(orig_tokens, tokens)\n",
    "\n",
    "        # Compare saliency for these words in Original vs Neutralized maps\n",
    "        orig_scores = ig_results[\"Original\"][1]\n",
    "        neutralized_saliency_comparison[cond] = {\n",
    "            \"Original avg\": sum(orig_scores[i] for i in neutralized_tokens)/len(neutralized_tokens) if neutralized_tokens else 0.0,\n",
    "            \"Neutralized avg\": sum(scores[i] for i in neutralized_tokens)/len(neutralized_tokens) if neutralized_tokens else 0.0\n",
    "        }\n",
    "\n",
    "    # Average saliency per group\n",
    "    saliency_summary[cond] = {\n",
    "        \"Label tokens\": sum(scores[i] for i in label_tokens)/len(label_tokens) if label_tokens else 0.0,\n",
    "        \"CE tokens\": sum(scores[i] for i in ce_tokens)/len(ce_tokens) if ce_tokens else 0.0,\n",
    "        \"Neutralized tokens\": sum(scores[i] for i in neutralized_tokens)/len(neutralized_tokens) if neutralized_tokens else 0.0\n",
    "    }\n",
    "\n",
    "    total_score = sum(scores)\n",
    "    percent_summary[cond] = {\n",
    "        \"Label %\": sum(scores[i] for i in label_tokens)/total_score*100 if total_score else 0.0,\n",
    "        \"CE %\": sum(scores[i] for i in ce_tokens)/total_score*100 if total_score else 0.0,\n",
    "        \"Neutralized %\": sum(scores[i] for i in neutralized_tokens)/total_score*100 if total_score else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "df_avg = pd.DataFrame(saliency_summary).T\n",
    "df_percent = pd.DataFrame(percent_summary).T\n",
    "df_neutralized = pd.DataFrame(neutralized_saliency_comparison).T\n",
    "\n",
    "print(\"Average IG saliency per token group:\")\n",
    "display(df_avg)\n",
    "\n",
    "print(\"IG saliency as % of total score:\")\n",
    "display(df_percent)\n",
    "\n",
    "print(\"Neutralized words saliency comparison (Original vs Neutralized):\")\n",
    "display(df_neutralized)\n",
    "\n",
    "\n",
    "### PLEASE BE CAREFUL TO SCROLL ALL THE WAY DOWN TO SEE THE RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4nGYJ4DCiFi"
   },
   "source": [
    "## 11. Captum LIME for BERT classifier\n",
    "\n",
    "Now use Captum LIME on DistilBERT to get perturbation-based saliency for the same example.\n",
    "\n",
    "### 11.1. Load BERT model + tokenizer for Captum\n",
    "\n",
    "We’ll load the same model that the pipeline used, but as AutoModelForSequenceClassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r4cmfOv9Ckox",
    "outputId": "04fd6784-ab15-4e12-bd3a-451b6e729919"
   },
   "outputs": [],
   "source": [
    "bert_model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(bert_model_name).to(device)\n",
    "bert_model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JnQh6OxCnzg"
   },
   "source": [
    "### 11.2. Forward function for Captum (returns logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjbARlPtCqPj"
   },
   "outputs": [],
   "source": [
    "def bert_forward_token_ids(input_ids: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "\n",
    "    outputs = bert_model(\n",
    "        input_ids=input_ids.to(device),\n",
    "        attention_mask=attention_mask.to(device) if attention_mask is not None else None,\n",
    "    )\n",
    "    return outputs.logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i931VvtCtlO"
   },
   "source": [
    "### 11.3. Prepare inputs & feature mask for the chosen example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I8tgLaSuCyFY",
    "outputId": "75499be1-34d6-4346-92a6-cdf9e22c42be"
   },
   "outputs": [],
   "source": [
    "test_text = example[\"sentence\"]\n",
    "true_label_name = \"Positive\" if example[\"label\"] == 1 else \"Negative\"\n",
    "\n",
    "print(\"LIME example text:\", test_text)\n",
    "print(\"True label:\", true_label_name)\n",
    "\n",
    "\n",
    "encoded = bert_tokenizer(test_text, return_tensors=\"pt\")\n",
    "input_ids_bert = encoded[\"input_ids\"].to(device)\n",
    "attention_mask_bert = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "seq_len = input_ids_bert.size(1)\n",
    "feature_mask = torch.arange(seq_len).unsqueeze(0).to(device)  # each token index is its own feature\n",
    "\n",
    "tokens_bert = bert_tokenizer.convert_ids_to_tokens(input_ids_bert[0])\n",
    "tokens_bert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kxvEe-pC3lb"
   },
   "source": [
    "### 11.4. Instantiate Captum LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6raC9g-yC4wq"
   },
   "outputs": [],
   "source": [
    "# Captum LIME -_> scikit-learn for the surrogate linear model\n",
    "#try:\n",
    "#    import sklearn  # noqa: F401\n",
    "#except ImportError:\n",
    "#    raise ImportError(\"scikit-learn for LIME; not present!!!!.\")\n",
    "\n",
    "\n",
    "similarity_func = get_exp_kernel_similarity_function()\n",
    "interpretable_model = SkLearnLinearModel(\"linear_model.LinearRegression\")\n",
    "\n",
    "lime = Lime(\n",
    "    forward_func=bert_forward_token_ids,\n",
    "    interpretable_model=interpretable_model,\n",
    "    similarity_func=similarity_func,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOJ3kULgDP-o"
   },
   "source": [
    "### 11.5. Compute LIME attributions\n",
    "\n",
    "Target class = 1 (Positive). You can also try 0 (Negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNMiqhSEDSW_"
   },
   "outputs": [],
   "source": [
    "target_class = 1  # Positive\n",
    "\n",
    "attr_lime = lime.attribute(\n",
    "    inputs=input_ids_bert,\n",
    "    additional_forward_args=(attention_mask_bert,),\n",
    "    target=target_class,\n",
    "    feature_mask=feature_mask,\n",
    "    n_samples=100,             # number of perturbations\n",
    "    perturbations_per_eval=10  # batch\n",
    ")  # shape: (1, seq_len)\n",
    "\n",
    "attr_lime.shape\n",
    "\n",
    "\n",
    "lime_scores = attr_lime[0].detach().cpu().numpy()\n",
    "abs_scores = np.abs(lime_scores)\n",
    "maxval = abs_scores.max() if abs_scores.max() != 0 else 1.0\n",
    "norm_lime = abs_scores / maxval\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osaMQX-KDYlJ"
   },
   "source": [
    "### 11.6. Visualize LIME tokens (simple intensity)\n",
    "\n",
    "You can either use sign (positive/negative) or just magnitude. Here we’ll color positive contributions green, negative red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "id": "cM_Blpx1DaxZ",
    "outputId": "b3861b4c-3881-4d23-fcb7-31fe1decd948"
   },
   "outputs": [],
   "source": [
    "def highlight_tokens_lime(tokens, scores, raw_scores):\n",
    "\n",
    "    spans = []\n",
    "    for tok, mag, val in zip(tokens, scores, raw_scores):\n",
    "        # skip special tokens\n",
    "        if tok in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]:\n",
    "            continue\n",
    "        if mag == 0:\n",
    "            spans.append(tok.replace(\"##\", \"\"))  # no highlight\n",
    "            spans.append(\" \")\n",
    "            continue\n",
    "\n",
    "        # color: green for positive, red for negative\n",
    "        if val >= 0:\n",
    "            color = f\"rgba(0, 255, 0, {mag})\"\n",
    "        else:\n",
    "            color = f\"rgba(255, 0, 0, {mag})\"\n",
    "\n",
    "        clean_tok = tok.replace(\"##\", \"\")  # join subwords\n",
    "        spans.append(f\"<span style='background-color:{color}'>{clean_tok}</span>\")\n",
    "        spans.append(\" \")\n",
    "\n",
    "    return \"\".join(spans)\n",
    "\n",
    "html_lime = highlight_tokens_lime(tokens_bert, norm_lime, lime_scores)\n",
    "display(HTML(f\"<h3>LIME explanation for BERT (target=Positive)</h3><div style='line-height:1.6em'>{html_lime}</div>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inG-LRZ49ja-"
   },
   "source": [
    "## 12. Testing Larger Models: e.g Qwen2.5-0.5B\n",
    "\n",
    "Prior examples can be easily reproduced on most local devices; however, LLMs are typically larger. The techniques discussed in the paper were tested on various model sizes. This section will demonstrate an example using **Qwen2.5-1.5B**.\"\n",
    "\n",
    "### Key Considerations for Larger Models:\n",
    "\n",
    "1. **Memory Requirements**: Larger models require more VRAM. Qwen2.5-1.5B needs ~8GB in fp16.\n",
    "2. **Tokenizer Differences**: Each model family has its own tokenizer with different special tokens.\n",
    "3. **Inference Speed**: Larger models are slower, affecting the time needed for attribution methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KPjUecgA1v3"
   },
   "source": [
    "### 12.1 Loading Qwen2.5-1.5B\n",
    "\n",
    "**Note**: The code below should NOT be executed if you don't have sufficient GPU resources if you want to read the trace first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277,
     "referenced_widgets": [
      "2c71e54681464443aac0428b48e93831",
      "053d3e1be09741d5a65f3370eaac798a",
      "c890f510af964c99a6b562e931d72bd8",
      "efd54715d2d44100b97b312b3476d606",
      "4b8c9c1759a14ced85a38dc22fc5ae7a",
      "ff2ffbfc5e4840bfbf1b1a8f7394eee2",
      "f71828521e354b7f9d11fd39ec29846d",
      "c995d9602b084f4b9c173cc153ad9430",
      "a80a044f34cc45079a94efceaf8047a1",
      "13e233656e1e4cd2b13c57d7a6155516",
      "8d1d305e8a93480892b0f41abfc36b34",
      "7faa0d6eec6a41b5ace55ae13d506b13",
      "c0309655c2e448e08fbf3ca397b9f6d4",
      "a4ef8ebf69204dbe858573ce4ec8f155",
      "f74710ba1c69470fb656743cf1f1fbbc",
      "d6bedc55d577441a92650775d2a2506b",
      "e51b15dcec5a4692816c4919efdd9220",
      "16c9b56a0ce54235a14ae426241b8ee7",
      "d82c0def6c6f4dc2a21cd752cdbff3b0",
      "e8f29088a12d41d8b66601236edf7e75",
      "ba92be97009a42f6948499882c7a08d0",
      "96df7bfa6ddb4700b37d9e03ff30d4ad",
      "7de728b0291042159c62da62e48ebbf0",
      "168d419a2747402aa7488781c81007cc",
      "60f7266532cf48758172025c59eb3627",
      "98065a3182c642ca828c761824361301",
      "56c7327174b94e71b7ffe3cbe48a3e15",
      "8dce0f0d51c347a687c4622706506e13",
      "9d9feba3b57e4a54b7e0c5c5c7e040f2",
      "316a7257e0a74dc8b5bfc368a1b14f42",
      "f1fddb51ce5448e2b321fe888552acd2",
      "9e994e2c5a884d49a9ebd02354cf2bf3",
      "ba041beeba2f4731a2a782e3102113a9",
      "99f29fb7ab24400fa911d10720ed768c",
      "8403c1a162954f19a3db49586fe0a3d3",
      "ef733f7e6d674137bf972069476e4233",
      "02dd576c4d234a579c70f5a8bd5bccda",
      "e49f488628404f628843b094b77a902d",
      "ad6530232f0b4d398545762e6e69325c",
      "293ccc458053453387899d3afb6557c4",
      "8cc59da3bcd648d789eebce421d40fce",
      "f0d092baaf9e4031a55ce180a2bfed2a",
      "4ac03afe438e4185a4e879ad747446ca",
      "cf5dcb8d2d6946869a56220819cc229e",
      "4de7eff3f95d4e04abb151069d928e95",
      "67687645bfa64b8a943ead4079fa7f44",
      "ad166e25851e47c19e843be523488a36",
      "49f667d171864e7da4c06cad49dc0f00",
      "eb47ec71510a44b386109a2ac25c1d2c",
      "527b0930432d44bb9584b6727f2c32d1",
      "337bfa3ae94b408c93e8a20235d992a0",
      "b18951306a0b4db1914f6a1df1ed5563",
      "bb15557da58f428c8be049e8f7c480cf",
      "1910e0a233244657b6cd2a177102ca12",
      "ec029f73239246f69951c0fa2d711015",
      "0eae5494bc2f476ebe0edb8f7da53e16",
      "b927387e3f3448c1b49146d7e8c6eb6b",
      "a483f0fcbf4b435ebbc9e9cdffedacef",
      "c96d8460bbac4e5e851fee8e3f70b981",
      "4d8c3edfb4894d0991f56b8966436aac",
      "4bd11647d0e448d88d648ca9bc3cd942",
      "5216b50a10e04f79b091a80fc1304205",
      "39fc7a12312740338b5e86ef56e3d96c",
      "38ebf03dc80f45c9bdb26d7e16c9dc54",
      "e9c53bbf7e524565845cd159c1911f5d",
      "e6898cdc36a4480b806e2695b75fbcba",
      "85fa58be2b994ed0ae88f05daa4798b2",
      "7a20dc0180b846eca5d616e0adc378d1",
      "e5b005ca91b54d34822d9834387ca84c",
      "dcad079884dc4efcb508d40f518795a2",
      "9822222b3eab45fe9d3b9d8f96e34bc7",
      "98f4357704754008b612e6ac62a5845d",
      "678ba525097f4d49acc659748d49b99f",
      "3d5b9c8e41a34b1580badfd1aa1067cd",
      "f91dc2da6a094f05995ff5ba309f34fc",
      "853700fa5f3948048873e3bf7dbe2b99",
      "35b08fcd5ee84ed58b8af39d1e394715"
     ]
    },
    "id": "O7g-8TiHBKkM",
    "outputId": "100cd3ed-0c3a-44c0-b7f3-0f554f097dc7"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load Qwen2.5-1.5B model and tokenizer\n",
    "qwen_model_name = \"Qwen/Qwen2.5-1.5B\"\n",
    "\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_model_name)\n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen_model_name,\n",
    "    torch_dtype=torch.float16,  # Use fp16 to reduce memory\n",
    "    device_map=\"auto\"           # Automatically distribute across available devices\n",
    ")\n",
    "qwen_model.eval()\n",
    "\n",
    "print(f\"Model loaded: {qwen_model_name}\")\n",
    "print(f\"Number of parameters: {qwen_model.num_parameters() / 1e9:.2f}B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GIGviRkkBZEy"
   },
   "source": [
    "### 12.2. Token Inference with Qwen2.5\n",
    "\n",
    "Unlike the simple next-token prediction used with GPT-2, larger models benefit from proper generation\n",
    "methods with sampling, temperature control, and stopping criteria.\n",
    "\n",
    "#### Documentation References:\n",
    "\n",
    "- **Hugging Face Generation Docs**: https://huggingface.co/docs/transformers/main/en/main_classes/text_generation\n",
    "- **Qwen2.5 Model Card**: https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct\n",
    "- **Generation Parameters**: https://huggingface.co/docs/transformers/main/en/generation_strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "w7jmpsDeBdbs",
    "outputId": "d445c1c9-79bd-4037-d3a8-8f697d8c8621"
   },
   "outputs": [],
   "source": [
    "def predict_qwen_sentiment(prompt: str, max_new_tokens: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Make Qwen generate a short label and parse it.\n",
    "    Mirrors the GPT-2 version but using Qwen's tokenizer/model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = qwen_tokenizer(prompt, return_tensors=\"pt\").to(qwen_model.device)\n",
    "\n",
    "    # Generate label\n",
    "    with torch.no_grad():\n",
    "        outputs = qwen_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=qwen_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode only new text\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    new_tokens = outputs[0][input_len:]\n",
    "    completion = qwen_tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "    return extract_label(completion)\n",
    "\n",
    "\n",
    "# Quick sanity test\n",
    "test_prompt = (\n",
    "    \"Review: I really loved this movie. It was exciting and beautiful.\\n\"\n",
    "    \"Sentiment (choose Positive or Negative): \"\n",
    ")\n",
    "print(\"Qwen2.5 test prediction should be positive:\", predict_qwen_sentiment(test_prompt))\n",
    "\n",
    "label_map = {\"Positive\": 1, \"Negative\": 0}\n",
    "conditions = [\"Original\", \"Label-Flipped\", \"Neutralized\", \"With-Explanation\", \"No-ICL\"]\n",
    "\n",
    "# We need to make a different prompt for Qwen because it needs reminder of who it is\n",
    "def build_prompt_qwen(demo_list, test_sentence: str) -> str:\n",
    "    prompt =\"You are Qwen, classifying a review's sentiment. It is either \\\"Positive\\\" or \\\"Negative\\\". Here are correct examples:\\n\"\n",
    "    for demo in demo_list:\n",
    "        prompt += demo.strip() + \"\\n\\n\"\n",
    "    prompt += f\"Review: {test_sentence.strip()}\\nSentiment: \"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "correct_counts_qwen = {c: 0 for c in conditions}\n",
    "all_preds_qwen = {c: [] for c in conditions}\n",
    "all_trues_qwen = []\n",
    "\n",
    "for ex in test_data:\n",
    "    true_label = ex[\"label\"]\n",
    "    sent = ex[\"sentence\"]\n",
    "\n",
    "    all_trues_qwen.append(true_label)\n",
    "\n",
    "    # EXACT same 5 prompt conditions as GPT-2\n",
    "    prompts = {\n",
    "        \"Original\":        build_prompt_qwen(original_demos, sent),\n",
    "        \"Label-Flipped\":   build_prompt_qwen(flipped_demos, sent),\n",
    "        \"Neutralized\":     build_prompt_qwen(neutral_demos, sent),\n",
    "        \"With-Explanation\": build_prompt_qwen(explained_demos, sent),\n",
    "        \"No-ICL\": (\n",
    "            \"You are Qwen, classifying a review's sentiment. \"\n",
    "            \"It is either \\\"Positive\\\" or \\\"Negative\\\".\\n\"\n",
    "            f\"Review: \\\"{sent}\\\"\\nSentiment: \"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    for cond, prompt in prompts.items():\n",
    "\n",
    "        pred_text = predict_qwen_sentiment(prompt)\n",
    "\n",
    "        if pred_text not in label_map:\n",
    "            print(f\"[Qwen] Unexpected prediction in '{cond}': {repr(pred_text)}\")\n",
    "            pred_label = None\n",
    "        else:\n",
    "            pred_label = label_map[pred_text]\n",
    "            all_preds_qwen[cond].append(pred_label)\n",
    "\n",
    "            if pred_label == true_label:\n",
    "                correct_counts_qwen[cond] += 1\n",
    "\n",
    "print(\"\\nQwen correct counts:\", correct_counts_qwen)\n",
    "\n",
    "accuracy_qwen = {\n",
    "    cond: correct_counts_qwen[cond] / len(test_data)\n",
    "    for cond in conditions\n",
    "}\n",
    "\n",
    "print(\"\\nQwen2.5 accuracy:\")\n",
    "for cond, acc in accuracy_qwen.items():\n",
    "    print(f\"{cond}: {acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "for cond in conditions:\n",
    "    cm = confusion_matrix(all_trues_qwen, all_preds_qwen[cond])\n",
    "\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(\n",
    "        cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "        xticklabels=[\"Negative\",\"Positive\"],\n",
    "        yticklabels=[\"Negative\",\"Positive\"]\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Qwen2.5 Confusion Matrix - {cond}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FA1slOEBlN7"
   },
   "source": [
    "### 12.3. Adapting Integrated Gradients for Qwen2.5\n",
    "\n",
    "The IG implementation needs modifications for larger models:\n",
    "\n",
    "1. **Different embedding layer**: Qwen uses `model.model.embed_tokens` instead of `model.transformer.wte`\n",
    "2. **Memory optimization**: Use gradient checkpointing and reduced batch sizes\n",
    "3. **Target token selection**: Identify the correct token IDs for \"Positive\"/\"Negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "z5Nwfx8wBp6K",
    "outputId": "6d6186e2-fe3d-4297-f999-9a4b16553575"
   },
   "outputs": [],
   "source": [
    "# Uncomment to use Integrated Gradients with Qwen2.5\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Get token IDs for sentiment labels\n",
    "positive_tokens = qwen_tokenizer.encode(\" Positive\", add_special_tokens=False)\n",
    "negative_tokens = qwen_tokenizer.encode(\" Negative\", add_special_tokens=False)\n",
    "print(f\"Positive token ID(s): {positive_tokens}\")\n",
    "print(f\"Negative token ID(s): {negative_tokens}\")\n",
    "\n",
    "# Use the first token ID for attribution\n",
    "positive_id = positive_tokens[0]\n",
    "\n",
    "\n",
    "\n",
    "def qwen_forward_for_ig(inputs_embeds: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "\n",
    "    outputs = qwen_model(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    # Get logits for the last position (next token prediction)\n",
    "    logits = outputs.logits[:, -1, :]  # (batch_size, vocab_size)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Return probability of positive token\n",
    "    return probs[:, positive_id]\n",
    "\n",
    "\n",
    "\n",
    "def encode_for_ig_qwen(prompt: str):\n",
    "\n",
    "    enc = qwen_tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = enc[\"input_ids\"].to(qwen_model.device)\n",
    "    attention_mask = enc[\"attention_mask\"].to(qwen_model.device)\n",
    "\n",
    "    # Get embeddings - Qwen uses model.model.embed_tokens\n",
    "    inputs_embeds = qwen_model.model.embed_tokens(input_ids)\n",
    "\n",
    "    return input_ids, inputs_embeds, attention_mask\n",
    "\n",
    "\n",
    "\n",
    "def ig_attribution_qwen(prompt: str, n_steps: int = 20):\n",
    "\n",
    "    input_ids, inputs_embeds, attention_mask = encode_for_ig_qwen(prompt)\n",
    "\n",
    "    # Create baseline (padding token)\n",
    "    baseline_ids = torch.full_like(input_ids, qwen_tokenizer.pad_token_id)\n",
    "    baseline_embeds = qwen_model.model.embed_tokens(baseline_ids)\n",
    "\n",
    "    # Initialize IG\n",
    "    ig = IntegratedGradients(qwen_forward_for_ig)\n",
    "\n",
    "    # Compute attributions\n",
    "    with torch.no_grad():\n",
    "        attr = ig.attribute(\n",
    "            inputs=inputs_embeds,\n",
    "            baselines=baseline_embeds,\n",
    "            additional_forward_args=(attention_mask,),\n",
    "            n_steps=n_steps,\n",
    "            internal_batch_size=1  # Reduce memory usage\n",
    "        )\n",
    "\n",
    "    # Sum over embedding dimension\n",
    "    attr_sum = attr.sum(dim=-1).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    # Normalize scores\n",
    "    abs_attr = np.abs(attr_sum)\n",
    "    max_val = abs_attr.max() if abs_attr.max() != 0 else 1.0\n",
    "    scores = abs_attr / max_val\n",
    "\n",
    "    # Get tokens\n",
    "    tokens = [qwen_tokenizer.decode([tid]) for tid in input_ids[0]]\n",
    "\n",
    "    return tokens, scores\n",
    "\n",
    "# Example usage:\n",
    "test_prompt = build_prompt(original_demos, test_data[0][\"sentence\"])\n",
    "tokens, scores = ig_attribution_qwen(test_prompt)\n",
    "html_str = highlight_tokens(tokens, scores)\n",
    "display(HTML(f\"\"\"\n",
    "    <div style='background-color:#f5f5f5; padding:15px; border-radius:5px; margin-bottom:20px;'>\n",
    "        <h3 style='color:#333; margin-top:0;'>(Example prompt saliency)</h3>\n",
    "        <div style='line-height:1.8em; font-size:14px;'>{html_str}</div>\n",
    "    </div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkPRpPWEB-ob"
   },
   "source": [
    "### 12.4. fit LIME for Qwen2.5\n",
    "\n",
    "LIME can be adapted similarly, but works directly with token IDs rather than embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "id": "DhXaO2IICFkG",
    "outputId": "4cc17183-4165-4e85-95c4-42c7a8716e90"
   },
   "outputs": [],
   "source": [
    "# Uncomment to use LIME with Qwen2.5 (for classification tasks)\n",
    "\n",
    "from captum.attr import Lime\n",
    "from captum.attr._core.lime import get_exp_kernel_similarity_function\n",
    "from captum._utils.models.linear_model import SkLearnLinearModel\n",
    "\n",
    "# Note: LIME is typically used with classification models that output class probabilities.\n",
    "# For causal LMs like Qwen2.5, you may need to adapt the forward function to return\n",
    "# classification scores for your task.\n",
    "\n",
    "\n",
    "def qwen_forward_for_lime(input_ids: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "\n",
    "    outputs = qwen_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    # Get logits for last position\n",
    "    logits = outputs.logits[:, -1, :]\n",
    "\n",
    "    # Return logits for positive and negative tokens only\n",
    "    # This simulates a binary classification output\n",
    "    sentiment_logits = torch.stack([\n",
    "        logits[:, negative_tokens[0]],\n",
    "        logits[:, positive_tokens[0]]\n",
    "    ], dim=1)\n",
    "\n",
    "    return sentiment_logits\n",
    "\n",
    "# Initialize LIME\n",
    "similarity_func = get_exp_kernel_similarity_function()\n",
    "interpretable_model = SkLearnLinearModel(\"linear_model.LinearRegression\")\n",
    "\n",
    "lime_qwen = Lime(\n",
    "    forward_func=qwen_forward_for_lime,\n",
    "    interpretable_model=interpretable_model,\n",
    "    similarity_func=similarity_func,\n",
    ")\n",
    "\n",
    "# Example usage:\n",
    "test_text = test_data[0][\"sentence\"]\n",
    "encoded = qwen_tokenizer(test_text, return_tensors=\"pt\")\n",
    "input_ids = encoded[\"input_ids\"].to(qwen_model.device)\n",
    "attention_mask = encoded[\"attention_mask\"].to(qwen_model.device)\n",
    "\n",
    "seq_len = input_ids.size(1)\n",
    "feature_mask = torch.arange(seq_len).unsqueeze(0).to(qwen_model.device)\n",
    "\n",
    "attr_lime = lime_qwen.attribute(\n",
    "    inputs=input_ids,\n",
    "    additional_forward_args=(attention_mask,),\n",
    "    target=1,  # Positive class\n",
    "    feature_mask=feature_mask,\n",
    "    n_samples=50,  # Reduced for speed\n",
    "    perturbations_per_eval=5\n",
    ")\n",
    "\n",
    "attr_lime.shape\n",
    "\n",
    "lime_scores = attr_lime[0].detach().cpu().numpy()\n",
    "abs_scores = np.abs(lime_scores)\n",
    "maxval = abs_scores.max() if abs_scores.max() != 0 else 1.0\n",
    "norm_lime = abs_scores / maxval\n",
    "\n",
    "html_lime = highlight_tokens_lime(tokens, norm_lime, lime_scores)\n",
    "display(HTML(f\"<h3>LIME explanation for GWEN (target=Positive)</h3><div style='line-height:1.6em'>{html_lime}</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MswKlwiICR20"
   },
   "source": [
    "### Useful Documentation:\n",
    "\n",
    "- **Qwen2.5 Documentation**: https://qwen.readthedocs.io/\n",
    "- **Hugging Face Text Generation**: https://huggingface.co/docs/transformers/main/en/main_classes/text_generation\n",
    "- **Captum Tutorials**: https://captum.ai/tutorials/\n",
    "- **Memory Optimization**: https://huggingface.co/docs/transformers/main/en/performance"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
